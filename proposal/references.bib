
@misc{boyle_spatiotemporal_2023,
	title = {Spatiotemporal {Transformer} for {Stock} {Movement} {Prediction}},
	url = {http://arxiv.org/abs/2305.03835},
	doi = {10.48550/arXiv.2305.03835},
	abstract = {Financial markets are an intriguing place that offer investors the potential to gain large profits if timed correctly. Unfortunately, the dynamic, non-linear nature of financial markets makes it extremely hard to predict future price movements. Within the US stock exchange, there are a countless number of factors that play a role in the price of a company's stock, including but not limited to financial statements, social and news sentiment, overall market sentiment, political happenings and trading psychology. Correlating these factors is virtually impossible for a human. Therefore, we propose STST, a novel approach using a Spatiotemporal Transformer-LSTM model for stock movement prediction. Our model obtains accuracies of 63.707 and 56.879 percent against the ACL18 and KDD17 datasets, respectively. In addition, our model was used in simulation to determine its real-life applicability. It obtained a minimum of 10.41\% higher profit than the S\&P500 stock index, with a minimum annualized return of 31.24\%.},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Boyle, Daniel and Kalita, Jugal},
	month = may,
	year = {2023},
	note = {arXiv:2305.03835 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computational Engineering, Finance, and Science},
}


@article{su_self-attentive_2022,
	title = {Self-{Attentive} {Moving} {Average} for {Time} {Series} {Prediction}},
	volume = {12},
	issn = {20763417},
	doi = {10.3390/app12073602},
	abstract = {Time series prediction has been studied for decades due to its potential in a wide range of applications. As one of the most popular technical indicators, moving average summarizes the overall changing patterns over a past period and is frequently used to predict the future trend of time series. However, traditional moving average indicators are calculated by averaging the time series data with equal or predefined weights, and ignore the subtle difference in the importance of different time steps. Moreover, unchanged data weights will be applied across different time series, regardless of the differences in their inherent characteristics. In addition, the interaction between different dimensions of different indicators is ignored when using the moving averages of different scales to predict future trends. In this paper, we propose a learning-based moving average indicator, called the self-attentive moving average (SAMA). After encoding the input signals of time series based on recurrent neural networks, we introduce the self-attention mechanism to adaptively determine the data weights at different time steps for calculating the moving average. Furthermore, we use multiple self-attention heads to model the SAMA indicators of different scales, and finally combine them through a bilinear fusion network for time series prediction. Extensive experiments on two real-world datasets demonstrate the effectiveness of our approach. The data and codes of our work have been released.},
	number = {7},
	journal = {Applied Sciences (2076-3417)},
	author = {Su, Yaxi and Cui, Chaoran and Qu, Hao},
	month = apr,
	year = {2022},
	keywords = {FORECASTING, moving average, MOVING average process, multi-scale indicator bilinear fusion, RECURRENT neural networks, self-attention mechanism, TIME series analysis, time series prediction},
	pages = {3602--3602},
}

@misc{kazemi_time2vec_2019,
	title = {{Time2Vec}: {Learning} a {Vector} {Representation} of {Time}},
	shorttitle = {{Time2Vec}},
	url = {http://arxiv.org/abs/1907.05321},
	doi = {10.48550/arXiv.1907.05321},
	abstract = {Time is an important feature in many applications involving events that occur synchronously and/or asynchronously. To effectively consume time information, recent studies have focused on designing new architectures. In this paper, we take an orthogonal but complementary approach by providing a model-agnostic vector representation for time, called Time2Vec, that can be easily imported into many existing and future architectures and improve their performances. We show on a range of models and problems that replacing the notion of time with its Time2Vec representation improves the performance of the final model.},
	urldate = {2024-10-05},
	publisher = {arXiv},
	author = {Kazemi, Seyed Mehran and Goel, Rishab and Eghbali, Sepehr and Ramanan, Janahan and Sahota, Jaspreet and Thakur, Sanjay and Wu, Stella and Smyth, Cathal and Poupart, Pascal and Brubaker, Marcus},
	month = jul,
	year = {2019},
	note = {arXiv:1907.05321 [cs]},
	keywords = {Computer Science - Machine Learning},
}


@misc{hasani_liquid_2020,
	title = {Liquid {Time}-constant {Networks}},
	url = {http://arxiv.org/abs/2006.04439},
	doi = {10.48550/arXiv.2006.04439},
	abstract = {We introduce a new class of time-continuous recurrent neural network models. Instead of declaring a learning system's dynamics by implicit nonlinearities, we construct networks of linear first-order dynamical systems modulated via nonlinear interlinked gates. The resulting models represent dynamical systems with varying (i.e., liquid) time-constants coupled to their hidden state, with outputs being computed by numerical differential equation solvers. These neural networks exhibit stable and bounded behavior, yield superior expressivity within the family of neural ordinary differential equations, and give rise to improved performance on time-series prediction tasks. To demonstrate these properties, we first take a theoretical approach to find bounds over their dynamics and compute their expressive power by the trajectory length measure in latent trajectory space. We then conduct a series of time-series prediction experiments to manifest the approximation capability of Liquid Time-Constant Networks (LTCs) compared to classical and modern RNNs. Code and data are available at https://github.com/raminmh/liquid\_time\_constant\_networks},
	urldate = {2024-10-07},
	publisher = {arXiv},
	author = {Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Rus, Daniela and Grosu, Radu},
	month = dec,
	year = {2020},
	note = {arXiv:2006.04439 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted to the Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)},
}
