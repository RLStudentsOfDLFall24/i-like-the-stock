mode:
  - train
  #- eval

model_types:
  #- rnn
  - transformer
  - lnn
  - lnn_cfc
  #- lnn_ncps

global_params:
  train_symbols:
  - "atnf"
  - "bivi"
  - "cycc"
  - "vtak"
  - "spx"
  target_symbol: "atnf"
  log_splits: False
  global_to_target_split: 0.82

rnn:
  hidden_size: 24
  dropout: 0.21
  num_layers: 2
  num_outputs: 3

rnn_trainer:
  batch_size: 20
  seq_len: 75
  epochs: 100
  lr: 5.e-6
  fine_tune_lr_ratio: 0.1
  optimizer:
    name: "adam"
    config:
      eps: 1.e-8
      betas: [0.9, 0.99]
      weight_decay: 1.e-3
  scheduler:
    name: 'plateau'
  criterion:
    name: "ce"

lnn:
  n_layers: 6
  hidden_size: 128
  output_size: 3
  activation: sigmoid

lnn_trainer:
  batch_size: 64
  seq_len: 20
  fine_tune_lr_ratio: 1.0
  epochs: 100
  lr: 1.e-3
  optimizer: 
    name: "adam"
    config:
      eps: 1.e-7
      betas: [0.9, 0.99]
      weight_decay: 0
  scheduler:
    name: 'plateau'
  criterion:
    name: "cb_focal"  # Class Balanced Focal Loss
  cbf_gamma: 1.0
  cbf_beta: 0.99

lnn_cfc:
  hidden_size: 64
  output_size: 3
  use_mixed: true # adds an LSTM into the mix
  backbone_dropout: 0.2
  backbone_layers: 3
  activation: lecun_tanh

lnn_cfc_trainer:
  batch_size: 64
  seq_len: 20
  fine_tune_lr_ratio: 0.1
  epochs: 100
  lr: 2.e-3
  optimizer: 
    name: "adam"
    config:
      eps: 1.e-7
      betas: [0.9, 0.99]
      weight_decay: 0
  scheduler:
    name: 'plateau'
  criterion:
    name: "cb_focal"  # Class Balanced Focal Loss
  cbf_gamma: 1.0
  cbf_beta: 0.99

lnn_ncps:
  n_layers: 6
  hidden_size: 32
  output_size: 3
  input_mapping: affine
  output_mapping: affine
  eps: 1.e-8
  use_mixed: false # adds an LSTM into the mix

lnn_ncps_trainer:
  batch_size: 16
  seq_len: 30
  fine_tune_lr_ratio: 0.1
  epochs: 200
  lr: 1.e-3
  optimizer: 
    name: "adam"
    config:
      eps: 1.e-7
      betas: [0.9, 0.99]
      weight_decay: 0
  scheduler:
    name: 'plateau'
  criterion:
    name: 'ce'

transformer:
  model_dim: 64
  seq_len: 20
  ignore_cols:
    - 0
  time_idx:
    - 6
    - 7
    - 8
  fc_dim: 1024
  fc_dropout: 0.1
  mlp_dim: 2048
  mlp_dropout: 0.3
  n_frequencies: 64
  num_encoders: 3
  num_heads: 8
  num_lstm_layers: 2
  lstm_dim: 256
  pretrained_t2v: data/t2v_weights/t2v_n64_mlp1024_lr6.310e-05.pth

transformer_trainer:
  batch_size: 128
  seq_len: 20
  epochs: 200
  lr: 5.e-5
  fine_tune_lr_ratio: 0.1
  optimizer:
    name: "adam"
    config:
      eps: 1.e-8
      betas: [0.9, 0.99]
      weight_decay: 1.e-3
  scheduler:
    name: 'plateau'
  criterion:
    name: "cb_focal"  # Class Balanced Focal Loss
  cbf_gamma: 1.0
  cbf_beta: 0.99
