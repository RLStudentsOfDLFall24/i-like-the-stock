{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset._dataset_utils import create_datasets\n",
    "\n",
    "\n",
    "ds_names = ['atnf', 'biaf', 'bivi', 'cycc', 'vtak']\n",
    "\n",
    "SEQ_LEN = 30\n",
    "LOG_SPLITS = False\n",
    "FIXED_SCALING = [(7, 3000.), (8, 12.), (9, 31.)]\n",
    "ROOT = './data/clean'\n",
    "\n",
    "datasets = { x: create_datasets(x, ROOT, FIXED_SCALING, LOG_SPLITS) for x in ds_names }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "BETAS = (0.9, 0.999)\n",
    "EPS = 1e-8\n",
    "WEIGHT_DECAY=1e-3\n",
    "\n",
    "GAMMA = 0.1\n",
    "STEP_SIZE = 0.1\n",
    "MILESTONES = [5, 10, 15]\n",
    "MIN_LR = 1e-7\n",
    "CRITERION_GAMMA=2.0\n",
    "\n",
    "OPTIMIZER = 'adam' # 'adam' or 'sgd'\n",
    "SCHEDULER = 'step' # 'plateau', 'step', or 'multi'\n",
    "CRITERION = 'ce'   # 'ce' or 'cb_focal'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_SIZE = 128\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "test = datasets\n",
    "\n",
    "data='atnf'\n",
    "\n",
    "input_size = datasets[data][0][0][0].shape[1]\n",
    "train_label_ct = datasets[data][0].target_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from src.cbfocal_loss import FocalLoss\n",
    "from src.models.abstract_model import AbstractModel\n",
    "\n",
    "\n",
    "def get_optimizer(type: str, model: AbstractModel, lr, **kwargs):\n",
    "  match type:\n",
    "    case 'adam':\n",
    "      return torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        **kwargs,\n",
    "      )\n",
    "    case 'sgd':\n",
    "      return torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        **kwargs,\n",
    "      )\n",
    "    case _:\n",
    "      raise ValueError(f'Unknown optimizer: {type}')\n",
    "  \n",
    "\n",
    "def get_scheduler(type: str, optimizer: torch.optim.Optimizer, **kwargs):\n",
    "  match type:\n",
    "    case 'plateau':\n",
    "      return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, **kwargs)\n",
    "    case 'step':\n",
    "      return torch.optim.lr_scheduler.StepLR(optimizer, **kwargs)\n",
    "    case 'multi':\n",
    "      return torch.optim.lr_scheduler.MultiStepLR(optimizer, **kwargs)\n",
    "    case _:\n",
    "      raise ValueError(f'Unknown scheduler: {type}')\n",
    "    \n",
    "\n",
    "def get_criterion(type: str, train_label_ct: Optional[torch.Tensor] = None, device='cpu', **kwargs):\n",
    "  match type:\n",
    "    case 'ce':\n",
    "      weight = None\n",
    "      if train_label_ct is not None:\n",
    "        weight = train_label_ct.max() / train_label_ct\n",
    "        weight = weight / weight.sum()\n",
    "        weight = weight.to(device)\n",
    "\n",
    "      return torch.nn.CrossEntropyLoss(\n",
    "        weight=weight,\n",
    "      )\n",
    "    case 'cb_focal':\n",
    "      return FocalLoss(\n",
    "        class_counts=train_label_ct.to(device),\n",
    "        **kwargs,\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.lnn import LNN\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "model = LNN(BATCH_SIZE, input_size, HIDDEN_SIZE)\n",
    "optimizer = get_optimizer(OPTIMIZER, model, LR, betas=BETAS, eps=EPS, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = get_scheduler(SCHEDULER, optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "criterion = get_criterion(CRITERION, train_label_ct, device)\n",
    "\n",
    "train, valid, test = datasets[data]\n",
    "\n",
    "train_loader, valid_loader, test_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True), DataLoader(valid, batch_size=BATCH_SIZE, shuffle=False), DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: AbstractModel,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.CrossEntropyLoss | FocalLoss,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    writer: SummaryWriter = None,\n",
    "  ):\n",
    "  model.train()\n",
    "\n",
    "  losses = np.zeros(len(dataloader))\n",
    "\n",
    "  for idx, data in enumerage(dataloader):\n",
    "    x = data[0].to(device)\n",
    "    y = data[1].to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(x)\n",
    "\n",
    "    if len(logits.shape) == 1:\n",
    "      logits = logits.unsqueeze(0)\n",
    "\n",
    "    loss = criterion(logits, y)\n",
    "\n",
    "    loss.backward()\n",
    "    th.nn.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if writer is not None:\n",
    "      for l, (name, param) in enumerate(model.named_parameters()):\n",
    "        if param.grad is not None:\n",
    "          writer.add_scalar(f'Gradients/{l:02}_{name}', param.grad.norm().item(), epoch * len(dataloader) + idx)\n",
    "\n",
    "    losses[idx] = loss.item()\n",
    "\n",
    "  return losses.sum(), losses.mean()\n",
    "\n",
    "train_losses = np.zeros(epochs)\n",
    "valid_losses = np.zeros(epochs)\n",
    "\n",
    "pb = tqdm(total=EPOCHS, desc=\"Epochs\")\n",
    "for epoch in range(EPOCHS):\n",
    "  train_loss, train_loss_avg = train(model, train_loader, optimizer, criterion, device, epoch, writer=None)\n",
    "  scheduler.step(train_loss)\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-a2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
