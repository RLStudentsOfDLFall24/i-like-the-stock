{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset._dataset_utils import create_datasets\n",
    "\n",
    "\n",
    "ds_names = ['atnf', 'biaf', 'bivi', 'cycc', 'vtak']\n",
    "\n",
    "SEQ_LEN = 30\n",
    "LOG_SPLITS = False\n",
    "FIXED_SCALING = [(7, 3000.), (8, 12.), (9, 31.)]\n",
    "ROOT = './data/clean'\n",
    "\n",
    "datasets = { x: create_datasets(x, ROOT, FIXED_SCALING, LOG_SPLITS) for x in ds_names }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 22])\n"
     ]
    }
   ],
   "source": [
    "LR = 6e-3\n",
    "BETAS = (0.9, 0.99)\n",
    "EPS = 1e-7\n",
    "WEIGHT_DECAY=0\n",
    "\n",
    "GAMMA = 0.1\n",
    "STEP_SIZE = 0.1\n",
    "MILESTONES = [5, 10, 15]\n",
    "MIN_LR = 1e-6\n",
    "CRITERION_GAMMA=3.0\n",
    "\n",
    "OPTIMIZER = 'adam' # 'adam' or 'sgd'\n",
    "SCHEDULER = 'multi' # 'plateau', 'step', or 'multi'\n",
    "CRITERION = 'ce'   # 'ce' or 'cb_focal'\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "HIDDEN_SIZE = 32\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "data='atnf'\n",
    "\n",
    "print(datasets[data][0][0][0].shape)\n",
    "\n",
    "input_size = datasets[data][0][0][0].shape[1]\n",
    "train_label_ct = datasets[data][0].target_counts\n",
    "add_loss_to_scheduler = SCHEDULER == 'plateau'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.models.lnn import LNN\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from training_tools.utils import get_criterion, get_optimizer, get_scheduler\n",
    "\n",
    "\n",
    "model = LNN(BATCH_SIZE, input_size, HIDDEN_SIZE, 3, n_layers=6, eps=EPS, activation='leaky_relu', device=device).to(device)\n",
    "optimizer = get_optimizer(OPTIMIZER, model, LR, config={ 'betas': BETAS, 'eps': EPS, 'weight_decay': WEIGHT_DECAY, 'amsgrad': True })\n",
    "scheduler = get_scheduler(SCHEDULER, optimizer, config={'milestones': MILESTONES})\n",
    "criterion = get_criterion(CRITERION, train_label_ct, {'cbf_gamma': CRITERION_GAMMA, 'cbf_beta': 0.9}, device)\n",
    "\n",
    "ds_train, ds_valid, ds_test = datasets[data]\n",
    "\n",
    "train_loader, valid_loader, test_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True), DataLoader(ds_valid, batch_size=BATCH_SIZE, shuffle=False), DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"../data/tensorboard/testing\")\n",
    "\n",
    "from training_tools import train, evaluate\n",
    "\n",
    "train_losses = np.zeros(EPOCHS)\n",
    "valid_losses = np.zeros(EPOCHS)\n",
    "\n",
    "#pb = tqdm(total=EPOCHS, desc=\"Epochs\")\n",
    "#pb.clear()\n",
    "#pb.reset()\n",
    "for epoch in range(EPOCHS):\n",
    "  train_loss, train_loss_avg = train(model, train_loader, optimizer, criterion, device, epoch, writer=None)\n",
    "  if add_loss_to_scheduler:\n",
    "      scheduler.step(train_loss)\n",
    "  else:\n",
    "      scheduler.step()\n",
    "\n",
    "  _, valid_loss_avg, _, _, v_pred_dist, _ = evaluate(model, valid_loader, criterion, device=device)\n",
    "\n",
    "  train_losses[epoch] = train_loss_avg\n",
    "  valid_losses[epoch] = valid_loss_avg\n",
    "\n",
    "  writer.add_scalar('Loss/train', train_loss_avg, epoch)\n",
    "  writer.add_scalar('Loss/valid', valid_loss_avg, epoch)\n",
    "\n",
    "  pred_string = ' - '.join([f'C{ix} {x:.3f}' for ix, x in enumerate(v_pred_dist)])\n",
    "  #pb.set_description(\n",
    "  #  f'E: {epoch + 1} | Train: {train_loss_avg:.4f} | Valid {valid_loss_avg:.4f} | V_Pred Dist: {pred_string}'\n",
    "  #)\n",
    "  #pb.update(1)\n",
    "  print(f'E: {epoch + 1} | Train: {train_loss_avg:.4f} | Valid {valid_loss_avg:.4f} | V_Pred Dist: {pred_string}')\n",
    "  parms = model.parameters()\n",
    "  test = parms\n",
    "\n",
    "test_loss, test_loss_avg, test_acc, test_f1, test_pred_dist, test_mcc = evaluate(\n",
    "    model,\n",
    "    test_loader,\n",
    "    criterion,\n",
    "    device=device,\n",
    ")\n",
    "writer.add_scalar(\"Loss/test\", test_loss_avg, EPOCHS)\n",
    "writer.add_scalar(\"Accuracy/test\", test_acc, EPOCHS)\n",
    "writer.add_scalar(\"F1/test\", test_f1, EPOCHS)\n",
    "writer.add_scalar(\"MCC/test\", test_mcc, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_pred_dist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dataset_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_target_distribution\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_results\n\u001b[1;32m----> 4\u001b[0m print_target_distribution([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtest_pred_dist\u001b[49m)])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_acc)\n\u001b[0;32m      8\u001b[0m plot_results(train_losses, valid_losses, EPOCHS)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_pred_dist' is not defined"
     ]
    }
   ],
   "source": [
    "from src.dataset._dataset_utils import print_target_distribution\n",
    "from training_tools import plot_results\n",
    "\n",
    "print_target_distribution([(\"Test\", test_pred_dist)])\n",
    "\n",
    "print(test_acc)\n",
    "\n",
    "plot_results(train_losses, valid_losses, EPOCHS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-group-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
