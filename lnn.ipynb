{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset._dataset_utils import create_datasets\n",
    "\n",
    "\n",
    "ds_names = ['atnf', 'biaf', 'bivi', 'cycc', 'vtak']\n",
    "\n",
    "SEQ_LEN = 30\n",
    "LOG_SPLITS = False\n",
    "FIXED_SCALING = [(7, 3000.), (8, 12.), (9, 31.)]\n",
    "ROOT = './data/clean'\n",
    "\n",
    "datasets = { x: create_datasets(x, ROOT, FIXED_SCALING, LOG_SPLITS) for x in ds_names }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 22])\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-4\n",
    "BETAS = (0.9, 0.999)\n",
    "EPS = 1e-8\n",
    "WEIGHT_DECAY=1e-3\n",
    "\n",
    "GAMMA = 0.1\n",
    "STEP_SIZE = 0.1\n",
    "MILESTONES = [5, 10, 15]\n",
    "MIN_LR = 1e-7\n",
    "CRITERION_GAMMA=2.0\n",
    "\n",
    "OPTIMIZER = 'adam' # 'adam' or 'sgd'\n",
    "SCHEDULER = 'step' # 'plateau', 'step', or 'multi'\n",
    "CRITERION = 'ce'   # 'ce' or 'cb_focal'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_SIZE = 128\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "data='atnf'\n",
    "\n",
    "print(datasets[data][0][0][0].shape)\n",
    "\n",
    "input_size = datasets[data][0][0][0].shape[1]\n",
    "train_label_ct = datasets[data][0].target_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from src.cbfocal_loss import FocalLoss\n",
    "from src.models.abstract_model import AbstractModel\n",
    "\n",
    "\n",
    "def get_optimizer(type: str, model: AbstractModel, lr, **kwargs):\n",
    "  match type:\n",
    "    case 'adam':\n",
    "      return torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        **kwargs,\n",
    "      )\n",
    "    case 'sgd':\n",
    "      return torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        **kwargs,\n",
    "      )\n",
    "    case _:\n",
    "      raise ValueError(f'Unknown optimizer: {type}')\n",
    "  \n",
    "\n",
    "def get_scheduler(type: str, optimizer: torch.optim.Optimizer, **kwargs):\n",
    "  match type:\n",
    "    case 'plateau':\n",
    "      return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, **kwargs)\n",
    "    case 'step':\n",
    "      return torch.optim.lr_scheduler.StepLR(optimizer, **kwargs)\n",
    "    case 'multi':\n",
    "      return torch.optim.lr_scheduler.MultiStepLR(optimizer, **kwargs)\n",
    "    case _:\n",
    "      raise ValueError(f'Unknown scheduler: {type}')\n",
    "    \n",
    "\n",
    "def get_criterion(type: str, train_label_ct: Optional[torch.Tensor] = None, device='cpu', **kwargs):\n",
    "  match type:\n",
    "    case 'ce':\n",
    "      weight = None\n",
    "      if train_label_ct is not None:\n",
    "        weight = train_label_ct.max() / train_label_ct\n",
    "        weight = weight / weight.sum()\n",
    "        weight = weight.to(device)\n",
    "\n",
    "      return torch.nn.CrossEntropyLoss(\n",
    "        weight=weight,\n",
    "      )\n",
    "    case 'cb_focal':\n",
    "      return FocalLoss(\n",
    "        class_counts=train_label_ct.to(device),\n",
    "        **kwargs,\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.models.lnn import LNN\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "model = LNN(BATCH_SIZE, input_size, HIDDEN_SIZE, 3).to(device)\n",
    "optimizer = get_optimizer(OPTIMIZER, model, LR, betas=BETAS, eps=EPS, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = get_scheduler(SCHEDULER, optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "criterion = get_criterion(CRITERION, train_label_ct, device)\n",
    "\n",
    "ds_train, ds_valid, ds_test = datasets[data]\n",
    "\n",
    "train_loader, valid_loader, test_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True), DataLoader(ds_valid, batch_size=BATCH_SIZE, shuffle=False), DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: AbstractModel,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.CrossEntropyLoss | FocalLoss,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    writer: SummaryWriter = None,\n",
    "  ):\n",
    "  model.train()\n",
    "\n",
    "  losses = np.zeros(len(dataloader))\n",
    "\n",
    "  for idx, data in enumerate(dataloader):\n",
    "    x = data[0].to(device)\n",
    "    y = data[1].to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits: torch.Tensor = model(x)\n",
    "\n",
    "    if len(logits.shape) == 1:\n",
    "      logits = logits.unsqueeze(0)\n",
    "\n",
    "    loss = criterion(logits, y)\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if writer is not None:\n",
    "      for l, (name, param) in enumerate(model.named_parameters()):\n",
    "        if param.grad is not None:\n",
    "          writer.add_scalar(f'Gradients/{l:02}_{name}', param.grad.norm().item(), epoch * len(dataloader) + idx)\n",
    "\n",
    "    losses[idx] = loss.item()\n",
    "\n",
    "  return losses.sum(), losses.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: AbstractModel,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: torch.optim.Optimizer,\n",
    "    device: torch.device = 'cpu',\n",
    "):\n",
    "  losses = np.zeros(len(dataloader))\n",
    "  accuracies = np.zeros(len(dataloader))\n",
    "\n",
    "  all_preds = []\n",
    "  all_labels = []\n",
    "  with torch.no_grad():\n",
    "    for idx, data in enumerate(dataloader):\n",
    "      x = data[0].to(device)\n",
    "      y = data[0].to(device)\n",
    "\n",
    "      logits = model(x)\n",
    "\n",
    "      preds = torch.argmax(logits, dim=1)\n",
    "      all_preds.append(preds)\n",
    "      all_labels.append(y)\n",
    "\n",
    "      losses[idx] = criterion(logits, y).item()\n",
    "      accuracies[idx] = torch.sum(torch.argmax(logits, dim=1) == y).item() / y.shape[0]\n",
    "\n",
    "  all_preds = torch.cat(all_preds).cpu()\n",
    "  all_labels = torch.cat(all_labels).cpu()\n",
    "\n",
    "  classes, counts = torch.unique(all_preds, return_counts=True)\n",
    "  pred_dist = torch.zeros(3)\n",
    "  pred_dist[classes] = counts / counts.sum()\n",
    "\n",
    "  f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "  return losses.sum(), losses.mean(), accuracies.mean(), f1_weighted, pred_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/20 [00:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"host_softmax\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m train_loss, train_loss_avg \u001b[38;5;241m=\u001b[39m train(model, train_loader, optimizer, criterion, device, epoch, writer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 10\u001b[0m _, valid_loss_avg, _, _, v_pred_dist \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m train_losses[epoch] \u001b[38;5;241m=\u001b[39m train_loss_avg\n\u001b[0;32m     13\u001b[0m valid_losses[epoch] \u001b[38;5;241m=\u001b[39m valid_loss_avg\n",
      "Cell \u001b[1;32mIn[18], line 26\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, dataloader, criterion, device)\u001b[0m\n\u001b[0;32m     23\u001b[0m     all_preds\u001b[38;5;241m.\u001b[39mappend(preds)\n\u001b[0;32m     24\u001b[0m     all_labels\u001b[38;5;241m.\u001b[39mappend(y)\n\u001b[1;32m---> 26\u001b[0m     losses[idx] \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     27\u001b[0m     accuracies[idx] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m y)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     29\u001b[0m all_preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_preds)\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[1;32mc:\\Users\\killy\\.conda\\envs\\dl-group-project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\killy\\.conda\\envs\\dl-group-project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\killy\\.conda\\envs\\dl-group-project\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\killy\\.conda\\envs\\dl-group-project\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"host_softmax\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "train_losses = np.zeros(EPOCHS)\n",
    "valid_losses = np.zeros(EPOCHS)\n",
    "writer = SummaryWriter(log_dir=f\"../data/tensorboard/testing\")\n",
    "\n",
    "pb = tqdm(total=EPOCHS, desc=\"Epochs\")\n",
    "for epoch in range(EPOCHS):\n",
    "  train_loss, train_loss_avg = train(model, train_loader, optimizer, criterion, device, epoch, writer=None)\n",
    "  scheduler.step()\n",
    "\n",
    "  _, valid_loss_avg, _, _, v_pred_dist = evaluate(model, valid_loader, criterion, device=device)\n",
    "\n",
    "  train_losses[epoch] = train_loss_avg\n",
    "  valid_losses[epoch] = valid_loss_avg\n",
    "\n",
    "  writer.add_scalar('Loss/train', train_loss_avg, epoch)\n",
    "  writer.add_scalar('Loss/valid', valid_loss_avg, epoch)\n",
    "\n",
    "  pred_string = ' - '.join([f'C{ix} {x:.3f}' for ix, x in enumerate(v_pred_dist)])\n",
    "  pb.set_description(\n",
    "    f'E: {epoch + 1} | Train: {train_loss_avg:.4f} | Valid {valid_loss_avg:.4f} | V_Pred Dist: {pred_string}'\n",
    "  )\n",
    "  pb.update(1)\n",
    "\n",
    "test_loss, test_loss_avg, test_acc, test_f1, test_pred_dist = evaluate(\n",
    "    model,\n",
    "    test_loader,\n",
    "    criterion,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.dataset._dataset_utils import print_target_distribution\n",
    "\n",
    "print_target_distribution([(\"Test\", test_pred_dist)])\n",
    "\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(valid_losses, label='Valid Loss')\n",
    "# Set y scale between 0.25 and 1.25\n",
    "plt.xlim(0, EPOCHS)\n",
    "plt.ylim(0.0, 2)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../figures/trial_testing_loss.png\")\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-group-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
