
@misc{Authors14,
 author = {Authors},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture  submission ID 324. Supplied as additional material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {Authors},
 title = {Frobnication tutorial},
 note = {Supplied as additional material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = {Journal of Foo},
volume = 12, 
number = 1, 
pages = {234--778}, 
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13, 
number = 1, 
pages = {234--778}, 
year = 2003
}

@misc{hasani_liquid_2020,
	title = {Liquid {Time}-constant {Networks}},
	url = {http://arxiv.org/abs/2006.04439},
	doi = {10.48550/arXiv.2006.04439},
	abstract = {We introduce a new class of time-continuous recurrent neural network models. Instead of declaring a learning system's dynamics by implicit nonlinearities, we construct networks of linear first-order dynamical systems modulated via nonlinear interlinked gates. The resulting models represent dynamical systems with varying (i.e., liquid) time-constants coupled to their hidden state, with outputs being computed by numerical differential equation solvers. These neural networks exhibit stable and bounded behavior, yield superior expressivity within the family of neural ordinary differential equations, and give rise to improved performance on time-series prediction tasks. To demonstrate these properties, we first take a theoretical approach to find bounds over their dynamics and compute their expressive power by the trajectory length measure in latent trajectory space. We then conduct a series of time-series prediction experiments to manifest the approximation capability of Liquid Time-Constant Networks (LTCs) compared to classical and modern RNNs. Code and data are available at https://github.com/raminmh/liquid\_time\_constant\_networks},
	urldate = {2024-10-07},
	publisher = {arXiv},
	author = {Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Rus, Daniela and Grosu, Radu},
	month = dec,
	year = {2020},
	note = {arXiv:2006.04439 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted to the Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)},
}

@article{LNN_Tutorial,
author = {Pavel Nakaznenko},
title = {Step-by-Step Guide to Building an LTC Liquid Neural Network from Scratch},
note = {https://github.com/KPEKEP/LTCtutorial/blob/main/LNN_LTC_Tutorial_Eng.ipynb},
year = 2024
}

@article{NCP_Github,
author = {Mathias Lechner, et al},
title = {ncps},
note = {https://github.com/mlech26l/ncps/},
year = 2024
}

@article{CfC_LTC,
author = {Hasani, R., Lechner, M., Amini, A. et al.},
title = {Closed-form continuous-time neural networks.},
journal = {Nature Machine Intelligence},
number = 4, 
pages = {992–-1003}, 
note = {https://doi.org/10.1038/s42256-022-00556-7},
year = 2022
}

@article{hasani2022liquid,
  title={Liquid Structural State-Space Models},
  author={Hasani, Ramin and Lechner, Mathias and Wang, Tsun-Huang and Chahine, Makram and Amini, Alexander and Rus, Daniela},
  journal={arXiv preprint arXiv:2209.12951},
  year={2022}
}

@article{zou_stock_2023,
	title = {Stock {Market} {Prediction} via {Deep} {Learning} {Techniques}: {A} {Survey}},
	url = {https://arxiv.org/abs/2212.12717},
	author = {Zou, Jinan and Zhao, Qingying and Jiao, Yang and Cao, Haiyao and Liu, Yanxi and Yan, Qingsen and Abbasnejad, Ehsan and Liu, Lingqiao and Shi, Javen Qinfeng},
	year = {2023},
	note = {\_eprint: 2212.12717},
}

@article{STT_Paper,
	title = {Spatiotemporal {Transformer} for {Stock} {Movement} {Prediction}},
	url = {http://arxiv.org/abs/2305.03835},
	doi = {10.48550/arXiv.2305.03835},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Boyle, Daniel and Kalita, Jugal},
	month = may,
	year = {2023},
	note = {arXiv:2305.03835 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computational Engineering, Finance, and Science},
}

@article{STLAT_sota,
	title = {Selective transfer learning with adversarial training for stock movement prediction},
	volume = {34},
	issn = {0954-0091},
	url = {https://doi.org/10.1080/09540091.2021.2021143},
	doi = {10.1080/09540091.2021.2021143},
	number = {1},
	urldate = {2024-10-07},
	journal = {Connection Science},
	author = {Li, Yang and Dai, Hong-Ning and Zheng, Zibin},
	month = dec,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/09540091.2021.2021143},
	pages = {492--510},
}

@article{time2vec,
	title = {{Time2Vec}: {Learning} a {Vector} {Representation} of {Time}},
	shorttitle = {{Time2Vec}},
	url = {http://arxiv.org/abs/1907.05321},
	doi = {10.48550/arXiv.1907.05321},
	urldate = {2024-10-05},
	publisher = {arXiv},
	author = {Kazemi, Seyed Mehran and Goel, Rishab and Eghbali, Sepehr and Ramanan, Janahan and Sahota, Jaspreet and Thakur, Sanjay and Wu, Stella and Smyth, Cathal and Poupart, Pascal and Brubaker, Marcus},
	month = jul,
	year = {2019},
	note = {arXiv:1907.05321 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{LSTM_paper,
	title = {Long {Short}-term {Memory}},
	volume = {9},
	doi = {10.1162/neco.1997.9.8.1735},
	journal = {Neural computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = dec,
	year = {1997},
	pages = {1735--80},
}

@misc{BERT,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	urldate = {2024-10-20},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805},
	keywords = {Computer Science - Computation and Language},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	urldate = {2024-10-19},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
}
