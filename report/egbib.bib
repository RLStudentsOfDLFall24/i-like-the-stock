
@misc{Authors14,
 author = {Authors},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture  submission ID 324. Supplied as additional material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {Authors},
 title = {Frobnication tutorial},
 note = {Supplied as additional material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = {Journal of Foo},
volume = 12, 
number = 1, 
pages = {234--778}, 
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13, 
number = 1, 
pages = {234--778}, 
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14, 
number = 1, 
pages = {234--778}, 
year = 2004
}

@article{LNN_Tutorial,
author = {Pavel Nakaznenko},
title = {Step-by-Step Guide to Building an LTC Liquid Neural Network from Scratch},
note = {https://github.com/KPEKEP/LTCtutorial/blob/main/LNN_LTC_Tutorial_Eng.ipynb},
year = 2024
}

@article{NCP_Github,
author = {Mathias Lechner, et al},
title = {ncps},
note = {https://github.com/mlech26l/ncps/},
year = 2024
}

@article{CfC_LTC,
author = {Hasani, R., Lechner, M., Amini, A. et al.},
title = {Closed-form continuous-time neural networks.},
journal = {Nature Machine Intelligence},
number = 4, 
pages = {992–-1003}, 
note = {https://doi.org/10.1038/s42256-022-00556-7},
year = 2022
}

@article{hasani2022liquid,
  title={Liquid Structural State-Space Models},
  author={Hasani, Ramin and Lechner, Mathias and Wang, Tsun-Huang and Chahine, Makram and Amini, Alexander and Rus, Daniela},
  journal={arXiv preprint arXiv:2209.12951},
  year={2022}
}

@article{zou_stock_2023,
	title = {Stock {Market} {Prediction} via {Deep} {Learning} {Techniques}: {A} {Survey}},
	url = {https://arxiv.org/abs/2212.12717},
	author = {Zou, Jinan and Zhao, Qingying and Jiao, Yang and Cao, Haiyao and Liu, Yanxi and Yan, Qingsen and Abbasnejad, Ehsan and Liu, Lingqiao and Shi, Javen Qinfeng},
	year = {2023},
	note = {\_eprint: 2212.12717},
}

@article{STT_Paper,
	title = {Spatiotemporal {Transformer} for {Stock} {Movement} {Prediction}},
	url = {http://arxiv.org/abs/2305.03835},
	doi = {10.48550/arXiv.2305.03835},
	abstract = {Financial markets are an intriguing place that offer investors the potential to gain large profits if timed correctly. Unfortunately, the dynamic, non-linear nature of financial markets makes it extremely hard to predict future price movements. Within the US stock exchange, there are a countless number of factors that play a role in the price of a company's stock, including but not limited to financial statements, social and news sentiment, overall market sentiment, political happenings and trading psychology. Correlating these factors is virtually impossible for a human. Therefore, we propose STST, a novel approach using a Spatiotemporal Transformer-LSTM model for stock movement prediction. Our model obtains accuracies of 63.707 and 56.879 percent against the ACL18 and KDD17 datasets, respectively. In addition, our model was used in simulation to determine its real-life applicability. It obtained a minimum of 10.41\% higher profit than the S\&P500 stock index, with a minimum annualized return of 31.24\%.},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Boyle, Daniel and Kalita, Jugal},
	month = may,
	year = {2023},
	note = {arXiv:2305.03835 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computational Engineering, Finance, and Science},
}

@article{STLAT_sota,
	title = {Selective transfer learning with adversarial training for stock movement prediction},
	volume = {34},
	issn = {0954-0091},
	url = {https://doi.org/10.1080/09540091.2021.2021143},
	doi = {10.1080/09540091.2021.2021143},
	abstract = {Stock movement prediction is a critical issue in the field of financial investment. It is very challenging since a stock usually shows highly stochastic property in price and has complex relationships with other stocks. Most existing approaches cannot jointly take the above two issues into account and thus cannot yield satisfactory prediction result. This paper contributes a new stock movement prediction model, Selective Transfer Learning with Adversarial Training (STLAT). Our STLAT method advances existing solutions in two major aspects: (i) tailoring the pre-trained and fine-tuned method for stock movement prediction and (ii) introducing the data selector module to select the more relevant training samples. More specifically, we pre-train the shared base model using three different tasks. The predictor task is constructed to measure the performance of the shared base model with source domain data and target domain data. The adversarial training task is constructed to improve the generalisation of the shared base model. The data selector task is introduced to select the most relevant and high-quality training samples from stocks in source domain. All three tasks are jointly trained with a loss function. As a result, the pre-trained shared base model can be fine-tuned with the stock data in target domain. To validate our method, we perform the back-testing on the historical data of two public datasets and a newly constructed dataset. Extensive experiments demonstrate the superiority of our STLAT method. It outperforms state-of-the-art stock prediction solutions on ACC evaluation of 3.76\%, 4.12\%, 4.89\% on ACL18, KDD17 and CN50, respectively.},
	number = {1},
	urldate = {2024-10-07},
	journal = {Connection Science},
	author = {Li, Yang and Dai, Hong-Ning and Zheng, Zibin},
	month = dec,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/09540091.2021.2021143},
	pages = {492--510},
}

@article{time2vec,
	title = {{Time2Vec}: {Learning} a {Vector} {Representation} of {Time}},
	shorttitle = {{Time2Vec}},
	url = {http://arxiv.org/abs/1907.05321},
	doi = {10.48550/arXiv.1907.05321},
	abstract = {Time is an important feature in many applications involving events that occur synchronously and/or asynchronously. To effectively consume time information, recent studies have focused on designing new architectures. In this paper, we take an orthogonal but complementary approach by providing a model-agnostic vector representation for time, called Time2Vec, that can be easily imported into many existing and future architectures and improve their performances. We show on a range of models and problems that replacing the notion of time with its Time2Vec representation improves the performance of the final model.},
	urldate = {2024-10-05},
	publisher = {arXiv},
	author = {Kazemi, Seyed Mehran and Goel, Rishab and Eghbali, Sepehr and Ramanan, Janahan and Sahota, Jaspreet and Thakur, Sanjay and Wu, Stella and Smyth, Cathal and Poupart, Pascal and Brubaker, Marcus},
	month = jul,
	year = {2019},
	note = {arXiv:1907.05321 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{LSTM_paper,
	title = {Long {Short}-term {Memory}},
	volume = {9},
	doi = {10.1162/neco.1997.9.8.1735},
	journal = {Neural computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = dec,
	year = {1997},
	pages = {1735--80},
}

@misc{BERT,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-10-20},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805},
	keywords = {Computer Science - Computation and Language},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-10-19},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
}
