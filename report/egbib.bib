
@misc{hasani_liquid_2020,
	title = {Liquid {Time}-constant {Networks}},
	url = {http://arxiv.org/abs/2006.04439},
	doi = {10.48550/arXiv.2006.04439},
	abstract = {We introduce a new class of time-continuous recurrent neural network models. Instead of declaring a learning system's dynamics by implicit nonlinearities, we construct networks of linear first-order dynamical systems modulated via nonlinear interlinked gates. The resulting models represent dynamical systems with varying (i.e., liquid) time-constants coupled to their hidden state, with outputs being computed by numerical differential equation solvers. These neural networks exhibit stable and bounded behavior, yield superior expressivity within the family of neural ordinary differential equations, and give rise to improved performance on time-series prediction tasks. To demonstrate these properties, we first take a theoretical approach to find bounds over their dynamics and compute their expressive power by the trajectory length measure in latent trajectory space. We then conduct a series of time-series prediction experiments to manifest the approximation capability of Liquid Time-Constant Networks (LTCs) compared to classical and modern RNNs. Code and data are available at https://github.com/raminmh/liquid\_time\_constant\_networks},
	urldate = {2024-10-07},
	publisher = {arXiv},
	author = {Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Rus, Daniela and Grosu, Radu},
	month = dec,
	year = {2020},
	note = {arXiv:2006.04439 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted to the Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)},
}

@misc{LNN_Tutorial,
author = {Pavel Nakaznenko},
title = {Step-by-Step Guide to Building an LTC Liquid Neural Network from Scratch},
note = {https://github.com/KPEKEP/LTCtutorial/blob/main/
        LNN\_LTC\_Tutorial\_Eng.ipynb},
year = 2024
}

@misc{NCP_Github,
author = {Mathias Lechner et al},
title = {ncps},
note = {https://github.com/mlech26l/ncps/},
year = 2024
}

@article{CfC_LTC,
author = {Hasani, R., Lechner and M. and Amini, A. et al.},
title = {Closed-form continuous-time neural networks.},
journal = {Nature Machine Intelligence},
volume = 4, 
pages = {992–-1003}, 
note = {https://doi.org/10.1038/s42256-022-00556-7},
year = {2022}
}

@article{hasani2022liquid,
  title={Liquid Structural State-Space Models},
  author={Hasani, Ramin and Lechner, Mathias and Wang, Tsun-Huang and Chahine, Makram and Amini, Alexander and Rus, Daniela},
  journal={arXiv preprint arXiv:2209.12951},
  year={2022}
}

@misc{zou_survey,
	title = {Stock {Market} {Prediction} via {Deep} {Learning} {Techniques}: {A} {Survey}},
	url = {https://arxiv.org/abs/2212.12717},
	author = {Zou, Jinan and Zhao, Qingying and Jiao, Yang and Cao, Haiyao and Liu, Yanxi and Yan, Qingsen and Abbasnejad, Ehsan and Liu, Lingqiao and Shi, Javen Qinfeng},
	year = {2023},
	note = {\_eprint: 2212.12717},
}

@misc{STT_Paper,
	title = {Spatiotemporal {Transformer} for {Stock} {Movement} {Prediction}},
	url = {http://arxiv.org/abs/2305.03835},
	doi = {10.48550/arXiv.2305.03835},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Boyle, Daniel and Kalita, Jugal},
	month = may,
	year = {2023},
	note = {arXiv:2305.03835 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computational Engineering, Finance, and Science},
}

@article{STLAT_sota,
	title = {Selective transfer learning with adversarial training for stock movement prediction},
	volume = {34},
	issn = {0954-0091},
	url = {https://doi.org/10.1080/09540091.2021.2021143},
	doi = {10.1080/09540091.2021.2021143},
	number = {1},
	urldate = {2024-10-07},
	journal = {Connection Science},
	author = {Li, Yang and Dai, Hong-Ning and Zheng, Zibin},
	month = dec,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/09540091.2021.2021143},
	pages = {492--510},
}

@misc{time2vec,
	title = {{Time2Vec}: {Learning} a {Vector} {Representation} of {Time}},
	shorttitle = {{Time2Vec}},
	url = {http://arxiv.org/abs/1907.05321},
	doi = {10.48550/arXiv.1907.05321},
	urldate = {2024-10-05},
	publisher = {arXiv},
	author = {Kazemi, Seyed Mehran and Goel, Rishab and Eghbali, Sepehr and Ramanan, Janahan and Sahota, Jaspreet and Thakur, Sanjay and Wu, Stella and Smyth, Cathal and Poupart, Pascal and Brubaker, Marcus},
	month = jul,
	year = {2019},
	note = {arXiv:1907.05321 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{LSTM_paper,
	title = {Long {Short}-term {Memory}},
	volume = {9},
	doi = {10.1162/neco.1997.9.8.1735},
	journal = {Neural computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = dec,
	year = {1997},
	pages = {1735--80},
}

@misc{BERT,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	urldate = {2024-10-20},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805},
	keywords = {Computer Science - Computation and Language},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	urldate = {2024-10-19},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
}


@misc{stock_charts,
	title = {Technical {Indicators} {\textbar} {ChartSchool} {\textbar} {StockCharts}.com},
	url = {https://chartschool.stockcharts.com/table-of-contents/technical-indicators-and-overlays/technical-indicators},
	urldate = {2024-12-11},
}

@article{cb_focal,
  author       = {Yin Cui and
                  Menglin Jia and
                  Tsung{-}Yi Lin and
                  Yang Song and
                  Serge J. Belongie},
  title        = {Class-Balanced Loss Based on Effective Number of Samples},
  journal      = {CoRR},
  volume       = {abs/1901.05555},
  year         = {2019},
  url          = {http://arxiv.org/abs/1901.05555},
  eprinttype    = {arXiv},
  eprint       = {1901.05555},
  timestamp    = {Tue, 08 Sep 2020 16:29:29 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1901-05555.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{yahoo_finance_api,
  author = {Yahoo! Inc.},
  title = {Yahoo Finance},
  howpublished = {\url{https://finance.yahoo.com}},
}

@misc{yfinance,
  author = {Ran Aroussi},
  title = {yfinance: Yahoo! Finance market data downloader},
  year = {2019},
  howpublished = {\url{https://github.com/ranaroussi/yfinance}},
}

@article{irnn_initialize,
  author       = {Quoc V. Le and
                  Navdeep Jaitly and
                  Geoffrey E. Hinton},
  title        = {A Simple Way to Initialize Recurrent Networks of Rectified Linear
                  Units},
  journal      = {CoRR},
  volume       = {abs/1504.00941},
  year         = {2015},
  url          = {http://arxiv.org/abs/1504.00941},
  eprinttype    = {arXiv},
  eprint       = {1504.00941},
  timestamp    = {Mon, 13 Aug 2018 16:48:41 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/LeJH15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{deep_learning_book,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
